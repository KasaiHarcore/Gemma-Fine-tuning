{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Gemma for Cultural Language Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must follow:\n",
    "\n",
    "3. Fine-Tuning Gemma 2\n",
    "- Training Configuration\n",
    "    - Hyperparameters\n",
    "        - Learning rate, batch size, number of epochs, optimizer choice.\n",
    "    - Training Procedures\n",
    "        - Outline the training loop and checkpointing.\n",
    "        - Discuss any custom training scripts or frameworks used.\n",
    "- Performance Enhancement Techniques\n",
    "    - Few-Shot Prompting\n",
    "        - Explain how few-shot examples were selected and formatted.\n",
    "    - Retrieval-Augmented Generation\n",
    "        - Describe integration with external knowledge bases, if any.\n",
    "    - Regularization Techniques\n",
    "        - Use of dropout, early stopping to prevent overfitting.\n",
    "- Challenges and Solutions\n",
    "    - Detail any obstacles encountered during fine-tuning.\n",
    "    - Explain how these issues were addressed.\n",
    "4. Inference and Evaluation\n",
    "- Running Inference\n",
    "        - Step-by-step guide on how to generate outputs using the fine-tuned model.\n",
    "        - Provide code snippets for loading the model and running predictions.\n",
    "- Evaluation Metrics\n",
    "    - Quantitative Metrics\n",
    "        - BLEU score, ROUGE, perplexity, or other relevant metrics.\n",
    "    - Qualitative Analysis\n",
    "        - Human evaluation methods, feedback from native speakers.\n",
    "- Results\n",
    "    - Present evaluation results with tables and charts.\n",
    "    - Discuss the model's performance in different aspects (fluency, accuracy, cultural relevance).\n",
    "- Testing with Additional Inputs\n",
    "    - Demonstrate the model's robustness with varied test cases.\n",
    "    - Analyze performance on edge cases or complex inputs.\n",
    "5. Reproducibility and Deployment\n",
    "\n",
    "Notebook Documentation\n",
    "Ensure all code cells are well-commented.\n",
    "Use markdown cells to explain each section and provide context.\n",
    "Publishing the Model\n",
    "Instructions for accessing the model on Kaggle Models.\n",
    "Include model versioning and any necessary metadata.\n",
    "Replication Steps\n",
    "Detailed guide for other users to replicate the fine-tuning process.\n",
    "Mention any default settings and how to modify them for other contexts.\n",
    "Inference Script\n",
    "Provide a standalone script or function for running inference.\n",
    "Explain input requirements and output formats.\n",
    "6. Exploring Cultural and Linguistic Nuances\n",
    "\n",
    "Language Fluency\n",
    "Techniques used to enhance fluency and naturalness of generated text.\n",
    "Addressing dialects or regional variations.\n",
    "Literary Traditions\n",
    "Adapting the model to generate or analyze poetry, proverbs, folklore.\n",
    "Include examples showcasing the model's capability in these areas.\n",
    "Historical Texts\n",
    "Handling archaic language or historical scripts.\n",
    "Methods used to train the model on historical data.\n",
    "7. Ethical Considerations and Cultural Sensitivity\n",
    "\n",
    "Bias Mitigation\n",
    "Steps taken to identify and reduce biases in the model.\n",
    "Cultural Respect\n",
    "Ensuring the model's outputs are culturally appropriate.\n",
    "Engagement with community experts or native speakers.\n",
    "Data Privacy\n",
    "Compliance with data protection laws and guidelines.\n",
    "Handling of any personal or sensitive information.\n",
    "8. Conclusion\n",
    "\n",
    "Summary of Achievements\n",
    "Recap the main accomplishments of the project.\n",
    "Limitations\n",
    "Discuss any limitations or areas where the model underperforms.\n",
    "Future Work\n",
    "Suggestions for further improvements or extensions.\n",
    "Potential for adapting the approach to other languages or contexts.\n",
    "Call to Action\n",
    "Encourage the community to build upon this work.\n",
    "Provide contact information for collaboration or feedback.\n",
    "9. References\n",
    "\n",
    "Cite all data sources, libraries, and frameworks used.\n",
    "Acknowledge any third-party contributions.\n",
    "10. Appendices (if applicable)\n",
    "\n",
    "Additional Code\n",
    "Include any supplementary scripts or functions.\n",
    "Extended Results\n",
    "Provide full evaluation reports or additional output examples.\n",
    "Glossary\n",
    "Define technical terms or language-specific concepts.\n",
    "Tips for Making the Notebook Community-Friendly:\n",
    "\n",
    "Clarity and Accessibility\n",
    "Use clear and concise language.\n",
    "Avoid jargon or explain it when necessary.\n",
    "Visual Aids\n",
    "Incorporate charts, graphs, and images to illustrate points.\n",
    "Interactive Elements\n",
    "Utilize Kaggle's interactive features for code execution.\n",
    "Engagement\n",
    "Pose questions or thought prompts to engage readers.\n",
    "Consistency\n",
    "Maintain a consistent style and formatting throughout the notebook.\n",
    "By following this outline, your notebook will not only meet all the competition requirements but also stand out in clarity, thoroughness, and community impact, increasing your chances of being among the top submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Table of Contents\n",
    "1. [Introduction](#1.)<br>\n",
    "2. [Library Import](#2.)<br>\n",
    "3. [Data Collection & Preprocessing](#3.)<br>\n",
    "4. [Model & Hyperparams](#4.)<br>\n",
    "&emsp;4.1 [Memory Requirement & Model Selection](#4.1)<br>\n",
    "&emsp;4.2 [Low Rank Adaption (LoRA)](#4.2)<br>\n",
    "&emsp;4.3 [Quantization](#4.3)<br>\n",
    "5. [Fine-tuning model](#5.)<br>\n",
    "6. [Model Evaluation](#6.)<br>\n",
    "&emsp;6.1 [Prompting Technique Evaluation](#6.2)<br>\n",
    "&emsp;6.2 [RAG for Advanced Nom Analysis](#6.3)<br>\n",
    "7. [Benchmark](#7.)<br>\n",
    "8. [Conclusion](#8.)<br>\n",
    "9. [References](#9.)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/KasaiHarcore/Gemma-Fine-tuning\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">1. Introduction </div> <a id = \"1.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, AI technologies such as chatbots and artificial intelligence-integrated support systems have been progressively integrated into various aspects of human life, particularly within the realm of natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this domain, fine-tuning and optimizing language models for specific languages, especially underrepresented or low-resource languages, holds immense significance. This is not just a technical challenge but also a mission to foster inclusivity and linguistic diversity in the digital age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"gemma2.jpg\" alt = \"Gemma Power\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this project focuses on enhancing [**Gemma 2**](https://huggingface.co/blog/gemma2#:~:text=Gemma%202%20Instruct%20has%20been,model%20oriented%20more%20towards%20conversational), Google’s latest open large language model (LLM) and a continuation of the Gemini framework by Google DeepMind. Designed for versatility and high performance, the model offers extensive capabilities, including an 8K-token context length, a broad training foundation, and a permissive license that supports diverse applications. Its adaptability makes it an ideal candidate for bridging linguistic and cultural gaps in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Nom-Tay.jpg\" alt = \"Internet Archive\">\n",
    "\n",
    "In this work, I aim to fine-tune Gemma 2 to better understand and generate text in [**Chữ Nôm**](https://en.wikipedia.org/wiki/Ch%E1%BB%AF_N%C3%B4m#:~:text=Ch%E1%BB%AF%20N%C3%B4m%20is%20the%20logographic,%E5%9C%8B%E8%AA%9E%2C%20'national%20language'), the traditional Vietnamese logographic script. Vietnamese Nôm, or Chữ Nôm, was an ancient writing system in Vietnam before the 20th century. It evolved from Chinese characters but adapted to Vietnamese sounds and vocabulary. Nôm was used by scholars for literature and communication. The script visually differed from Chinese characters and expressed Vietnamese concepts with semantic and phonetic components [**(read more)**](https://www.quora.com/How-was-the-Han-Nom-Chu-Nom-script-different-from-the-Chinese-script-Was-it-ineffective-for-Vietnamese). Today, Chữ Nôm is a specialized field, and efforts are made to preserve its knowledge. Though modern Vietnamese uses the Latin alphabet, Nôm remains an integral part of Vietnam's cultural heritage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adapting Gemma 2 to this context, the project aspires to breathe new life into this ancient script and make it accessible in AI-driven applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I will focus on in this notebook:\n",
    "- Enhancing the fluency and cultural relevance of AI.\n",
    "- Preserving and facilitating access to traditional literary heritage.\n",
    "- Expanding the capabilities of AI to work with logographic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some requirements libraries can be used for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers nltk trl huggingface_hub watermark matplotlib seaborn peft --quiet\n",
    "!pip install -U bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">2. Library Import</div> <a id = \"2.\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nguye\\anaconda3\\envs\\ai\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Python version in this Jupyter Notebook: 3.12.0\n",
      "Author: Library versions\n",
      "\n",
      "transformers: 4.44.2\n",
      "watermark   : 2.5.0\n",
      "matplotlib  : 3.9.0\n",
      "numpy       : 1.26.4\n",
      "json        : 2.0.9\n",
      "pandas      : 2.2.2\n",
      "torch       : 2.3.1+cu121\n",
      "re          : 2.2.1\n",
      "platform    : 1.0.8\n",
      "seaborn     : 0.13.2\n",
      "nltk        : 3.8.1\n",
      "sklearn     : 1.5.0\n",
      "peft        : 0.12.0\n",
      "bs4         : 4.12.3\n",
      "datasets    : 3.0.0\n",
      "trl         : 0.11.0\n",
      "requests    : 2.32.3\n",
      "bitsandbytes: 0.44.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# NLP processing\n",
    "import nltk\n",
    "import unicodedata\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Python version\n",
    "from platform import python_version\n",
    "print('Python version in this Jupyter Notebook:', python_version())\n",
    "\n",
    "# Load library versions\n",
    "import watermark\n",
    "\n",
    "# Library versions\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Library versions\" --iversions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 25 03:08:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.14                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090      WDDM  |   00000000:05:00.0 Off |                  N/A |\n",
      "| 55%   36C    P8             33W /  390W |       0MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# My specs for the GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Google already public their model on Hugging Face, so we can use it directly. But we will need to loggin to use it. You can create HuggingFace account then follow this instruction:\n",
    "- [Access Token](https://huggingface.co/docs/hub/security-tokens)\n",
    "- [Quick Start](https://huggingface.co/docs/huggingface_hub/quick-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\nguye\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "tk = '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' # Your token goes here\n",
    "\n",
    "login(token = tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">3. Data Collection & Preprocessing</div> <a id = \"3.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because most of the Nôm language comes from old text script in poems, history docs, or literature, we can use some of the old text to fine-tune the model. The dataset can be found:\n",
    "- [**Sentences set**](https://www.kaggle.com/datasets/quandang/nomnanmt.) \n",
    "- [**Poetry set**](https://chunom.org/shelf/corpus./)\n",
    "- Custom Nôm dictionary with correct Vietnamese mean translation that I manually process based on [**Vietnamese Nôm Preservation Foundation**](https://nomfoundation.org/) for evaluating the quality of the data and ensuring cultural sensitivity.\n",
    "\n",
    "Data also available on [**HuggingFace**](https://huggingface.co/datasets/KasaiDanto/vietnamese_nom_scripts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nom_vi(file_path: str, output_path: str = r\"./data/output.csv\") -> pd.DataFrame:\n",
    "    \"\"\" Split the text file data \"\"\"\n",
    "    nom = []\n",
    "    vi = []\n",
    "    \n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        # Split by tab\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) == 2:\n",
    "            nom.append(parts[0].strip())\n",
    "            vi.append(parts[1].strip())\n",
    "        else:\n",
    "            print(f\"Process error at line number {line}\")\n",
    "    \n",
    "    # Create DataFrame and saving\n",
    "    data = pd.DataFrame({\"nom\": nom, \"vi\": vi})\n",
    "    data.to_csv(output_path, index = False, encoding = \"utf-8\")\n",
    "    print(f\"Data Finish at {output_path}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Finish at ./data/sentences\\DVSKTT-1 Quyen thu.csv\n",
      "Data Finish at ./data/sentences\\DVSKTT-2 Ngoai ky toan thu.csv\n",
      "Data Finish at ./data/sentences\\DVSKTT-3 Ban ky toan thu.csv\n",
      "Data Finish at ./data/sentences\\DVSKTT-4 Ban ky thuc luc.csv\n",
      "Data Finish at ./data/sentences\\DVSKTT-5 Ban ky tuc bien.csv\n",
      "Data Finish at ./data/sentences\\Luc Van Tien.csv\n",
      "Data Finish at ./data/sentences\\Tale of Kieu 1866.csv\n",
      "Data Finish at ./data/sentences\\Tale of Kieu 1871.csv\n",
      "Data Finish at ./data/sentences\\Tale of Kieu 1872.csv\n"
     ]
    }
   ],
   "source": [
    "data_path = os.walk(\"./data/sentences\")\n",
    "for path, _, files in data_path:\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            output_path = os.path.join(path, file.replace(\".txt\", \".csv\"))\n",
    "            split_nom_vi(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sens = pd.DataFrame()\n",
    "data_path = os.walk(\"./data/sentences\")\n",
    "for path, _, files in data_path:\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            dataset_sens = pd.concat([dataset_sens, pd.read_csv(file_path)])\n",
    "            \n",
    "dataset_sens = dataset_sens.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/poetry/nom.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    nom = f.readlines()\n",
    "    \n",
    "with open(\"./data/poetry/vi.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    vi = f.readlines()\n",
    "    \n",
    "dataset_poe = pd.DataFrame({\"nom\": nom, \"vi\": vi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom</th>\n",
       "      <th>vi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>阿</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>阿片</td>\n",
       "      <td>a phiến</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>阿彌陀佛</td>\n",
       "      <td>a di đà phật</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>埃</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>安</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>英</td>\n",
       "      <td>anh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>英</td>\n",
       "      <td>anh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>英姉㛪</td>\n",
       "      <td>anh chị em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>英户</td>\n",
       "      <td>anh họ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>𠀧</td>\n",
       "      <td>ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>爸</td>\n",
       "      <td>ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>爸媽</td>\n",
       "      <td>ba má</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>班</td>\n",
       "      <td>ban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>班𣈜</td>\n",
       "      <td>ban ngày</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>班創</td>\n",
       "      <td>ban sáng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>班头</td>\n",
       "      <td>ban đầu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>邦</td>\n",
       "      <td>bang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>包</td>\n",
       "      <td>bao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>包紙</td>\n",
       "      <td>bao giấy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>包𣇞</td>\n",
       "      <td>bao giờ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     nom            vi\n",
       "0      阿             a\n",
       "1     阿片       a phiến\n",
       "2   阿彌陀佛  a di đà phật\n",
       "3      埃            ai\n",
       "4      安            an\n",
       "5      英           anh\n",
       "6      英           anh\n",
       "7    英姉㛪    anh chị em\n",
       "8     英户        anh họ\n",
       "9      𠀧            ba\n",
       "10     爸            ba\n",
       "11    爸媽         ba má\n",
       "12     班           ban\n",
       "13    班𣈜      ban ngày\n",
       "14    班創      ban sáng\n",
       "15    班头       ban đầu\n",
       "16     邦          bang\n",
       "17     包           bao\n",
       "18    包紙      bao giấy\n",
       "19    包𣇞       bao giờ"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = pd.read_csv('./data/Nom_dictionary.csv')\n",
    "dataset_dict.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, depend how the language is, we will have different preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vi_text(text: str) -> list:\n",
    "    # Normalize text\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nom_text(text: str) -> list:\n",
    "    # Remove latin characters\n",
    "    text = re.sub(r'[a-z]', '', text)\n",
    "    # Remove extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_csv = pd.concat([dataset_sens, dataset_poe, dataset_dict], ignore_index = True)\n",
    "dataset_csv = dataset_csv.drop_duplicates()\n",
    "dataset_csv = dataset_csv.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocess to nom and vi\n",
    "dataset_csv['nom'] = dataset_csv['nom'].apply(preprocess_nom_text)\n",
    "dataset_csv['vi'] = dataset_csv['vi'].apply(preprocess_vi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(dataset_csv, test_size = 0.1, random_state = 42)\n",
    "val, test = train_test_split(val, test_size = 0.5, random_state = 42)\n",
    "\n",
    "train.to_csv('./data/train.csv', index = False)\n",
    "val.to_csv('./data/val.csv', index = False)\n",
    "test.to_csv('./data/test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8648635c240548ef85c9a21e3d11ea2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f35c9ab1a24bd7b2c576009c3809f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a20d449a09445cca61c7058be93bc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files = {'train': './data/train.csv', 'test': './data/test.csv', 'validation': './data/val.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">4. Model Loading & Hyperparams Tuning</div> <a id = \"4.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Memory Checking & Model Selection <a id = \"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize performance and the ability to produce the best results, I use the Gemma 2 model with 2B parameters. Along with that, most people (especially those who are new to LLM) will have difficulty finding and choosing the right model that fit their own specs, so I want to share a website that can help you with this, which is [**LLM Checker**](https://rahulschand.github.io/gpu_poor/). (**Note that below image is an example**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"llmcheck.png\" alt = \"LLM Model Choosing\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it may not give you 100% accurate results, but it will give you a good starting point. Now let us check what Gemma can do with our task. See this [**guide**](https://huggingface.co/docs/transformers/conversations) if you want to customize your own prompt or using any advanced method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this cell code I only run one time and it separate from all the other to prevent GPU memory exceed\n",
    "\n",
    "def chat_with_model(model_name: str, user_input: str, max_length: int = 1024, trained_lora: str = None, bnb_config: BitsAndBytesConfig = BitsAndBytesConfig(load_in_8bit = True)):\n",
    "    \"\"\" Function to load a text generation model and simulate a chat interaction \"\"\"\n",
    "    try:\n",
    "        # Load the tokenizer and model\n",
    "        print(f\"Loading model and tokenizer for '{model_name}'...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config = bnb_config)\n",
    "        if trained_lora is not None:\n",
    "            model = PeftModel.from_pretrained(model, trained_lora)\n",
    "        \n",
    "        # Tokenize user input\n",
    "        print(\"Generating response... \\n\")\n",
    "        inputs = tokenizer(user_input, return_tensors = \"pt\")\n",
    "        \n",
    "        # Generate a response\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_length = max_length, do_sample = True, top_k = 10, top_p = 0.95)\n",
    "        \n",
    "        # Decode and return the response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "        print(f\"Response: {response} \\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "input_text = f\"Translate this '{dataset_csv['nom'].iloc[4]}' to Vietnamese and explain the context meaning\"\n",
    "chat_with_model(model_name, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the model's response, it seems that Gemma did a pretty good job at understanding the context, but if based on the linguistic meaning in Vietnamese, the translated sentence could be adjusted to more accurately reflect the style and the spirit of ancient literature. The model may give the general idea of ​​history's continuity and incompleteness but does not convey the significance and depth of the original sentence, which is rich in historical culture and the semantic style of ancient literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Low Rank Adaption <a id = \"4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of trainable parameters is greatly decreased by the well-liked and portable training method known as LoRA (Low-Rank Adaptation of Large Language Models). It functions by adding fewer new weights to the model, and only these are used for training.\n",
    "\n",
    "Therefore, instead of loading the entire model into the GPU, backpropagating the entire model, and updating all of its weights, fine-tuning LoRA generates two more matrices, A and B, on top of the model's initial weights, W, which are frozen. When these two matrices, A and B, are multiplied together, a new matrix with the same dimensions as the original weight matrix, W, is produced. The loss is only backpropagated during the training process after it has been calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"lora.png\" alt = \"LoRA\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any Math lover who want to explore this, I recommend you read [**this**](https://medium.com/@lokeshtodwal/demystifying-lora-q-lora-ea267abff48) article to understand how it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 16, # Rank\n",
    "    lora_alpha = 32, # Adjusting Coefficient\n",
    "    lora_dropout = 0.1, # Chance to skip LoRA using\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Quantization <a id = \"4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, quantization is a technique that lowers the accuracy of a model's parameters. With the popularity of Large Langue Models (LLMs), this method gained popularity. Using this method, 32-bit or 16-bit floating-point numbers are converted to lower precision formats like 8-bit or 4-bit values. This method's main objective is to reduce the model's size in order to lower the computational requirements.\n",
    "\n",
    "Quantization is important for the following main reasons:\n",
    "- **Reduced Model Size**: Lowering the precision of the weights and quantization can significantly reduce the storage requirements. This is particularly beneficial for deploying models on devices with limited memory on edge devices.\n",
    "- **Faster Inference**: Quantized models require less computational power, and that leads to faster inference times. This is crucial for real-time applications and user experience.\n",
    "- **Lower Power Consumption**: Reduced computational requirements translate to lower power consumption; this will be very important for battery-operated devices.\n",
    "- **Scalability**: A smaller model makes the deployment easier and scalability easier.\n",
    "- **Cost Efficiency**: Smaller models with a high accuracy mean less cost for the same number of requests. This makes large language models accessible to smaller organizations and startup\n",
    "\n",
    "More complicated stuff [**here**](https://www.maartengrootendorst.com/blog/quantization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_use_double_quant = True,\n",
    "#     bnb_4bit_quant_type = \"nf4\",\n",
    "#     bnb_4bit_compute_dtype = torch.bfloat16\n",
    "# )\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally some pre-defined params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "OUTPUT_DIR = \"./working/model\"\n",
    "\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">5. Fine-tuning model</div> <a id = \"5.\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_function(examples: pd.DataFrame) -> pd.DataFrame:\n",
    "    input = examples['nom']\n",
    "    output = examples['vi']\n",
    "    intruction_template = f\"\"\"\n",
    "    **Instruction:**\n",
    "\n",
    "    Please translate the following text to Vietnamese.\n",
    "\n",
    "    **Text to Translate:**\n",
    "    {input}\n",
    "    \n",
    "    **Response:**\n",
    "    {output}\n",
    "    \"\"\"\n",
    "    \n",
    "    return {'prompt': intruction_template}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309209756f6848f5b70e93f84287f6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55cce3cb1e2450aa2445d0c08ba8041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7beb6fcf385a4d4fa74855b045ee7f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_dataset = dataset.map(formatting_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\" Prints the number of trainable parameters in the model. \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d55946ac68643479a7be693e2728838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config = bnb_config, device_map = \"auto\", attn_implementation = 'eager')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6389760 || all params: 2620731648 || trainable%: 0.2438158826706396\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(get_peft_model(model, lora_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87fc0877c8d45f59066a3fd3daa28a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31297 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8844bdc4688a4fa79aa25fb73e2829de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1739 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset = formatted_dataset[\"train\"],\n",
    "    eval_dataset = formatted_dataset[\"validation\"],\n",
    "    args = TrainingArguments(\n",
    "        num_train_epochs = EPOCHS,\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        per_device_eval_batch_size = BATCH_SIZE,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        learning_rate = 5e-5,\n",
    "        weight_decay = 0.001,\n",
    "        adam_beta1 = 0.9,\n",
    "        adam_beta2 = 0.995,\n",
    "        adam_epsilon = 1e-8,\n",
    "        max_grad_norm = 1.0,\n",
    "        seed = 4856,\n",
    "        output_dir = OUTPUT_DIR,\n",
    "        optim = \"adamw_bnb_8bit\",\n",
    "        lr_scheduler_type = \"reduce_lr_on_plateau\"\n",
    "    ),\n",
    "    peft_config = lora_config,\n",
    "    dataset_text_field = \"prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a99a04121a84730b87a41ff7b593f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0863, 'grad_norm': 3.7867989540100098, 'learning_rate': 5e-05, 'epoch': 0.06}\n",
      "{'loss': 1.8223, 'grad_norm': 8.309208869934082, 'learning_rate': 5e-05, 'epoch': 0.13}\n",
      "{'loss': 1.6827, 'grad_norm': 4.461440563201904, 'learning_rate': 5e-05, 'epoch': 0.19}\n",
      "{'loss': 1.5995, 'grad_norm': 3.8560783863067627, 'learning_rate': 5e-05, 'epoch': 0.26}\n",
      "{'loss': 1.5734, 'grad_norm': 4.111842632293701, 'learning_rate': 5e-05, 'epoch': 0.32}\n",
      "{'loss': 1.5077, 'grad_norm': 5.331409454345703, 'learning_rate': 5e-05, 'epoch': 0.38}\n",
      "{'loss': 1.4931, 'grad_norm': 3.3308260440826416, 'learning_rate': 5e-05, 'epoch': 0.45}\n",
      "{'loss': 1.4615, 'grad_norm': 3.298192024230957, 'learning_rate': 5e-05, 'epoch': 0.51}\n",
      "{'loss': 1.4425, 'grad_norm': 3.2954165935516357, 'learning_rate': 5e-05, 'epoch': 0.58}\n",
      "{'loss': 1.4192, 'grad_norm': 4.4286065101623535, 'learning_rate': 5e-05, 'epoch': 0.64}\n",
      "{'loss': 1.3961, 'grad_norm': 6.291783332824707, 'learning_rate': 5e-05, 'epoch': 0.7}\n",
      "{'loss': 1.4016, 'grad_norm': 4.430474281311035, 'learning_rate': 5e-05, 'epoch': 0.77}\n",
      "{'loss': 1.3732, 'grad_norm': 3.5374033451080322, 'learning_rate': 5e-05, 'epoch': 0.83}\n",
      "{'loss': 1.3544, 'grad_norm': 4.3185272216796875, 'learning_rate': 5e-05, 'epoch': 0.89}\n",
      "{'loss': 1.3446, 'grad_norm': 3.372077465057373, 'learning_rate': 5e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b52e96465c413a95efa11c14f40601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3377771377563477, 'eval_runtime': 223.6673, 'eval_samples_per_second': 7.775, 'eval_steps_per_second': 1.945, 'epoch': 1.0}\n",
      "{'loss': 1.34, 'grad_norm': 3.314457416534424, 'learning_rate': 5e-05, 'epoch': 1.02}\n",
      "{'loss': 1.3003, 'grad_norm': 3.4284536838531494, 'learning_rate': 5e-05, 'epoch': 1.09}\n",
      "{'loss': 1.2939, 'grad_norm': 2.8539879322052, 'learning_rate': 5e-05, 'epoch': 1.15}\n",
      "{'loss': 1.2861, 'grad_norm': 3.207547664642334, 'learning_rate': 5e-05, 'epoch': 1.21}\n",
      "{'loss': 1.3051, 'grad_norm': 9.829144477844238, 'learning_rate': 5e-05, 'epoch': 1.28}\n",
      "{'loss': 1.2926, 'grad_norm': 2.627249240875244, 'learning_rate': 5e-05, 'epoch': 1.34}\n",
      "{'loss': 1.2857, 'grad_norm': 3.291450262069702, 'learning_rate': 5e-05, 'epoch': 1.41}\n",
      "{'loss': 1.2782, 'grad_norm': 4.691164493560791, 'learning_rate': 5e-05, 'epoch': 1.47}\n",
      "{'loss': 1.2873, 'grad_norm': 2.9475576877593994, 'learning_rate': 5e-05, 'epoch': 1.53}\n",
      "{'loss': 1.2476, 'grad_norm': 19.61113929748535, 'learning_rate': 5e-05, 'epoch': 1.6}\n",
      "{'loss': 1.2463, 'grad_norm': 2.529611587524414, 'learning_rate': 5e-05, 'epoch': 1.66}\n",
      "{'loss': 1.26, 'grad_norm': 3.6795215606689453, 'learning_rate': 5e-05, 'epoch': 1.73}\n",
      "{'loss': 1.2666, 'grad_norm': 3.1373097896575928, 'learning_rate': 5e-05, 'epoch': 1.79}\n",
      "{'loss': 1.2476, 'grad_norm': 2.764058828353882, 'learning_rate': 5e-05, 'epoch': 1.85}\n",
      "{'loss': 1.2331, 'grad_norm': 2.9557721614837646, 'learning_rate': 5e-05, 'epoch': 1.92}\n",
      "{'loss': 1.2434, 'grad_norm': 2.6690196990966797, 'learning_rate': 5e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65b40a603b347a68d0f2e31b071afc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.252960205078125, 'eval_runtime': 237.5987, 'eval_samples_per_second': 7.319, 'eval_steps_per_second': 1.831, 'epoch': 2.0}\n",
      "{'loss': 1.2066, 'grad_norm': 3.174567937850952, 'learning_rate': 5e-05, 'epoch': 2.04}\n",
      "{'loss': 1.1961, 'grad_norm': 3.890836238861084, 'learning_rate': 5e-05, 'epoch': 2.11}\n",
      "{'loss': 1.1951, 'grad_norm': 2.9104647636413574, 'learning_rate': 5e-05, 'epoch': 2.17}\n",
      "{'loss': 1.2048, 'grad_norm': 5.512338161468506, 'learning_rate': 5e-05, 'epoch': 2.24}\n",
      "{'loss': 1.1974, 'grad_norm': 2.656956672668457, 'learning_rate': 5e-05, 'epoch': 2.3}\n",
      "{'loss': 1.1843, 'grad_norm': 2.8766958713531494, 'learning_rate': 5e-05, 'epoch': 2.36}\n",
      "{'loss': 1.1958, 'grad_norm': 2.3810036182403564, 'learning_rate': 5e-05, 'epoch': 2.43}\n",
      "{'loss': 1.1947, 'grad_norm': 2.6805338859558105, 'learning_rate': 5e-05, 'epoch': 2.49}\n",
      "{'loss': 1.1925, 'grad_norm': 2.990164041519165, 'learning_rate': 5e-05, 'epoch': 2.56}\n",
      "{'loss': 1.1798, 'grad_norm': 2.560637950897217, 'learning_rate': 5e-05, 'epoch': 2.62}\n",
      "{'loss': 1.186, 'grad_norm': 2.785802125930786, 'learning_rate': 5e-05, 'epoch': 2.68}\n",
      "{'loss': 1.1855, 'grad_norm': 2.6139590740203857, 'learning_rate': 5e-05, 'epoch': 2.75}\n",
      "{'loss': 1.179, 'grad_norm': 3.035477638244629, 'learning_rate': 5e-05, 'epoch': 2.81}\n",
      "{'loss': 1.1873, 'grad_norm': 2.836357831954956, 'learning_rate': 5e-05, 'epoch': 2.88}\n",
      "{'loss': 1.151, 'grad_norm': 3.1408939361572266, 'learning_rate': 5e-05, 'epoch': 2.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cac34c57ee4dbdb0fffc7f33cc3904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2114449739456177, 'eval_runtime': 243.9981, 'eval_samples_per_second': 7.127, 'eval_steps_per_second': 1.783, 'epoch': 3.0}\n",
      "{'loss': 1.1791, 'grad_norm': 3.1127374172210693, 'learning_rate': 5e-05, 'epoch': 3.0}\n",
      "{'loss': 1.129, 'grad_norm': 4.226780414581299, 'learning_rate': 5e-05, 'epoch': 3.07}\n",
      "{'loss': 1.1303, 'grad_norm': 3.3289742469787598, 'learning_rate': 5e-05, 'epoch': 3.13}\n",
      "{'loss': 1.1033, 'grad_norm': 2.9881794452667236, 'learning_rate': 5e-05, 'epoch': 3.19}\n",
      "{'loss': 1.1331, 'grad_norm': 3.9584014415740967, 'learning_rate': 5e-05, 'epoch': 3.26}\n",
      "{'loss': 1.1377, 'grad_norm': 3.102477788925171, 'learning_rate': 5e-05, 'epoch': 3.32}\n",
      "{'loss': 1.1514, 'grad_norm': 2.9609925746917725, 'learning_rate': 5e-05, 'epoch': 3.39}\n",
      "{'loss': 1.1321, 'grad_norm': 3.2523648738861084, 'learning_rate': 5e-05, 'epoch': 3.45}\n",
      "{'loss': 1.1259, 'grad_norm': 3.1557865142822266, 'learning_rate': 5e-05, 'epoch': 3.51}\n",
      "{'loss': 1.1465, 'grad_norm': 10.836472511291504, 'learning_rate': 5e-05, 'epoch': 3.58}\n",
      "{'loss': 1.1266, 'grad_norm': 3.056459903717041, 'learning_rate': 5e-05, 'epoch': 3.64}\n",
      "{'loss': 1.1306, 'grad_norm': 3.295457124710083, 'learning_rate': 5e-05, 'epoch': 3.71}\n",
      "{'loss': 1.1392, 'grad_norm': 3.205690383911133, 'learning_rate': 5e-05, 'epoch': 3.77}\n",
      "{'loss': 1.1377, 'grad_norm': 3.199432134628296, 'learning_rate': 5e-05, 'epoch': 3.83}\n",
      "{'loss': 1.1348, 'grad_norm': 3.255472183227539, 'learning_rate': 5e-05, 'epoch': 3.9}\n",
      "{'loss': 1.1341, 'grad_norm': 3.6126410961151123, 'learning_rate': 5e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2530236d9e4e89a1c8b29fc561bd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1880388259887695, 'eval_runtime': 238.6713, 'eval_samples_per_second': 7.286, 'eval_steps_per_second': 1.823, 'epoch': 4.0}\n",
      "{'loss': 1.1231, 'grad_norm': 3.273221492767334, 'learning_rate': 5e-05, 'epoch': 4.03}\n",
      "{'loss': 1.0955, 'grad_norm': 3.9406962394714355, 'learning_rate': 5e-05, 'epoch': 4.09}\n",
      "{'loss': 1.0677, 'grad_norm': 2.9625065326690674, 'learning_rate': 5e-05, 'epoch': 4.15}\n",
      "{'loss': 1.0699, 'grad_norm': 2.9987339973449707, 'learning_rate': 5e-05, 'epoch': 4.22}\n",
      "{'loss': 1.0744, 'grad_norm': 5.666394233703613, 'learning_rate': 5e-05, 'epoch': 4.28}\n",
      "{'loss': 1.0752, 'grad_norm': 3.0551328659057617, 'learning_rate': 5e-05, 'epoch': 4.35}\n",
      "{'loss': 1.106, 'grad_norm': 2.844954013824463, 'learning_rate': 5e-05, 'epoch': 4.41}\n",
      "{'loss': 1.1075, 'grad_norm': 2.907071352005005, 'learning_rate': 5e-05, 'epoch': 4.47}\n",
      "{'loss': 1.0733, 'grad_norm': 2.8475306034088135, 'learning_rate': 5e-05, 'epoch': 4.54}\n",
      "{'loss': 1.0948, 'grad_norm': 2.5725722312927246, 'learning_rate': 5e-05, 'epoch': 4.6}\n",
      "{'loss': 1.0913, 'grad_norm': 3.6597604751586914, 'learning_rate': 5e-05, 'epoch': 4.66}\n",
      "{'loss': 1.0724, 'grad_norm': 3.4263598918914795, 'learning_rate': 5e-05, 'epoch': 4.73}\n",
      "{'loss': 1.0811, 'grad_norm': 2.750040054321289, 'learning_rate': 5e-05, 'epoch': 4.79}\n",
      "{'loss': 1.08, 'grad_norm': 3.057884693145752, 'learning_rate': 5e-05, 'epoch': 4.86}\n",
      "{'loss': 1.1049, 'grad_norm': 3.277588129043579, 'learning_rate': 5e-05, 'epoch': 4.92}\n",
      "{'loss': 1.0739, 'grad_norm': 2.604203701019287, 'learning_rate': 5e-05, 'epoch': 4.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea5ac0f60ec4799aef4663bd405ab3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.171726942062378, 'eval_runtime': 241.6382, 'eval_samples_per_second': 7.197, 'eval_steps_per_second': 1.8, 'epoch': 5.0}\n",
      "{'train_runtime': 51442.2444, 'train_samples_per_second': 3.042, 'train_steps_per_second': 0.761, 'train_loss': 1.240223369561826, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39125, training_loss=1.240223369561826, metrics={'train_runtime': 51442.2444, 'train_samples_per_second': 3.042, 'train_steps_per_second': 0.761, 'total_flos': 1.8569537465717146e+17, 'train_loss': 1.240223369561826, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">6. Model Evaluation</div> <a id = \"6.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Prompting Technique Evaluation <a id = \"6.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<start_of_turn>system\\nUse the following information and your own knowledge to answer the question. If you don't know the answer, say you don't know, don't try to make up the answer.\\n\n",
    "    {context}<end_of_turn>\\n<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>assistant\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 RAG for Advanced Nom Analysis <a id = \"6.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Embedding model\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "#\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "#\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_from_text(strs: str):\n",
    "    \"\"\" Create a vector database from the text \"\"\"\n",
    "\n",
    "    # Initialize the text splitter\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator = \"\\n\",\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 50,\n",
    "        length_function = len\n",
    "    )\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(strs)\n",
    "    \n",
    "    # Load the embeddings model\n",
    "    embeddings_model = GPT4AllEmbeddings(model_file = \"model/all-MiniLM-L12-v2.Q8_0.gguf\")\n",
    "    \n",
    "    # Create the vector database with FAISS\n",
    "    database = FAISS.from_text(chunks, embeddings_model)\n",
    "    database.save_local(vector_db_path)\n",
    "    \n",
    "    return database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will using RAG to improve how Gemma can reason and explain Nom script using [**this**](https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems?uiLang=en) as a main source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_box(url: List[str]) -> str:\n",
    "    try:\n",
    "        # Fetch the content from url\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Parse the content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all div elements with class \"box\"\n",
    "    box_elements = soup.find_all('div', class_ = 'box')\n",
    "\n",
    "    # Extract text from the box elements\n",
    "    extracted_text = url + \"\\n\\n\".join([box.get_text(separator = ' ', strip = True) for box in box_elements])\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "data = []\n",
    "url = [\"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/1-Autumn%20Landscape?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/3-Offering%20betel?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/4-Confession%20(II)?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/5-Lament%20for%20the%20Prefect%20of%20V%C4%A9nh-T%C6%B0%E1%BB%9Dng?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/6-Lament%20for%20Commissioner%20C%C3%B3c?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/7-Confession%20(III)?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/8-The%20Floating%20Cake?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/9-On%20Sharing%20a%20Husband?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/10-Jackfruit?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/11-River%20Snail?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/13-Teasing%20Chi%C3%AAu-H%E1%BB%95?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/14-Chi%C3%AAu-H%E1%BB%95%E2%80%99s%20Reply?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/15-Three-Mountain%20Pass?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/18-The%20Unwed%20Mother?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/23-Picking%20Flowers?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/27-The%20Pharmacist%E2%80%99s%20Widow%20Mourns%20His%20Death?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/30-The%20Retired%20Doctor?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/32-Qu%C3%A1n%20S%C3%BA%20Pagoda?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/33-Buddhist%20Nun?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/34-The%20Lustful%20Monk?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/38-Tr%E1%BA%A5n%20Qu%E1%BB%91c%20Temple?uiLang=en\"\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/49-Spring%20%E2%80%93%20watching%20pavilion?uiLang=en\"\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/48-Country%20Scene?uiLang=en\",\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/39-At%20the%20Chinese%20General%E2%80%99s%20Tomb?uiLang=en\"\n",
    "        \"https://www.nomfoundation.org/nom-project/Ho-Xuan-Huong/Ho-Xuan-Huong-of-poems/42-The%20Crab?uiLang=en\"]\n",
    "for i in url:\n",
    "    box_content = extract_text_from_box(i)\n",
    "    data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_lang(pt: str, llm_model):\n",
    "    chain = RetrievalQA.from_chain_type(llm = llm_model,\n",
    "                                        retriever = VectorStoreRetriever(vectorstore = create_db_from_text(data)),\n",
    "                                        memory = ConversationSummaryMemory(llm = llm_model),\n",
    "                                        chain_type_kwargs = {\"prompt\": pt, \"verbose\": True},\n",
    "                                        return_source_documents = False\n",
    "                                        )\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">7. Benchmark</div> <a id = \"7.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare performance, I will using using this [website](https://www.clc.hcmus.edu.vn/?page_id=3039), which already been certified by competent authorities in Vietnam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">8. Conclusion</div> <a id = \"8.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:14px;color:white;margin:0;font-family:Georgia;font-size:30px;text-align:left;display:fill;border-radius:5px;background-color:#004AAD;overflow:hidden\">9. References</div> <a id = \"9.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hu, E.J. et al. (2021) Lora: Low-rank adaptation of large language models, arXiv.org. Available at: https://arxiv.org/abs/2106.09685 (Accessed: 16 November 2024). \n",
    "\n",
    "Dinh, D., Nguyen, P. and Nguyen, L.H.B. (no date) Transliterating Nôm scripts into Vietnamese National Scripts Using Statistical Machine Translation, International Journal of Advanced Computer Science and Applications (IJACSA). Available at: https://thesai.org/Publications/ViewPaper?Volume=12&Issue=2&Code=IJACSA&SerialNo=5 (Accessed: 16 November 2024). \n",
    "\n",
    "Gemma 2: Improving open language models at a practical ... Available at: http://arxiv.org/pdf/2408.00118 (Accessed: 16 November 2024). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
